{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BSSC-QA Framework: Synthetic Question-Answer Generation Pipeline\n",
        "\n",
        "**BSSC-QA** is a modular framework for generating high-quality question-answer pairs from text documents using multi-agent orchestration and RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "## Core Components:\n",
        "- **Document Processor**: Loads and cleans text documents\n",
        "- **Chunker**: Splits documents into semantic chunks\n",
        "- **Vector Store**: Indexes chunks for retrieval (ChromaDB + embeddings)\n",
        "- **Generator Agent**: Creates questions from chunks\n",
        "- **Synthesis Agent**: Generates evidence-based answers\n",
        "- **Evaluator Agent**: Assesses QA quality with multi-metric scoring\n",
        "- **Pipeline Orchestrator**: Coordinates the entire workflow\n",
        "\n",
        "This demo uses 4 Gutenberg (https://www.gutenberg.org/) novels to showcase the complete pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected papers:\n",
            "  - G K Chesterton___The Man Who Knew Too Much.txt (326.7 KB)\n",
            "  - Herbert Spencer___Essays on Education and Kindred Subjects.txt (876.6 KB)\n",
            "  - Jack London___The Faith of Men.txt (258.8 KB)\n",
            "  - Rudyard Kipling___The Jungle Book.txt (272.1 KB)\n",
            "Workspace root: /home/kaizu/Projects/test/BSSC_QA\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys, os, shutil\n",
        "import textwrap\n",
        "\n",
        "PROJECT_ROOT = Path(\"/home/kaizu/Projects/test/BSSC_QA\")  # Change to BSSC-QA directory\n",
        "if not (PROJECT_ROOT / \"bssc_qa\").exists():\n",
        "    print(\"Error: BSSC-QA directory not found.\")\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "SRC_PATH = PROJECT_ROOT / \"bssc_qa\" / \"src\"\n",
        "if str(SRC_PATH) not in sys.path:\n",
        "    sys.path.append(str(SRC_PATH))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"papers\"\n",
        "paper_paths = sorted(DATA_DIR.glob(\"*.txt\"))[:4]\n",
        "\n",
        "print(\"Selected papers:\")\n",
        "for path in paper_paths:\n",
        "    size_kb = path.stat().st_size / 1024\n",
        "    print(f\"  - {path.name} ({size_kb:.1f} KB)\")\n",
        "print(f\"Workspace root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab434594",
      "metadata": {},
      "source": [
        "### Create/Edit a new config\n",
        "\n",
        "BSSC-QA uses a centralized JSON config with these key sections:\n",
        "\n",
        "**LLM Providers**: Multi-provider support (Gemini, DeepSeek, Mistral, HuggingFace)\n",
        "- Each agent can use a different provider\n",
        "- Configurable temperature and model selection\n",
        "\n",
        "**Vector Store**: ChromaDB with customizable embeddings\n",
        "- `offline-hash`: Fast, deterministic hashing\n",
        "- `sentence-transformers`: Semantic embeddings\n",
        "\n",
        "**Chunking Strategy**: Adaptive text splitting\n",
        "- Default: 512 tokens with 50-token overlap\n",
        "- Auto-adjusts based on model context window\n",
        "\n",
        "**Agent Configuration**:\n",
        "- Generator: LLM provider + retry logic\n",
        "- Synthesis: Context window size + evidence span limits\n",
        "- Evaluator: Quality threshold + custom metrics\n",
        "\n",
        "*Note: The config is already loaded for this demo. Uncomment the cell below to create/modify your own.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa285f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# # Default configuration\n",
        "# config = {\n",
        "#     \"llm\": {                                                                                    # LLM provider settings (You can add or remove providers)\n",
        "#         \"default_provider\": \"gemini\",\n",
        "#         \"providers\": {\n",
        "#             \"gemini\": {                                       \n",
        "#                 \"api_key\": \"your_api_key_here\",\n",
        "#                 \"model\": \"gemini-2.5-flash\",\n",
        "#                 \"temperature\": 0.7\n",
        "#             },\n",
        "#             \"deepseek\": {\n",
        "#                 \"api_key\": \"your_api_key_here\",\n",
        "#                 \"model\": \"deepseek-chat\",\n",
        "#                 \"temperature\": 0.7\n",
        "#             },\n",
        "#             \"mistral\": {\n",
        "#                 \"api_key\": \"your_api_key_here\",\n",
        "#                 \"model\": \"mistral-large-latest\",\n",
        "#                 \"temperature\": 0.7\n",
        "#             },\n",
        "#             \"huggingface\": {\n",
        "#                 \"api_key\": \"your_api_key_here\",\n",
        "#                 \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "#                 \"temperature\": 0.7\n",
        "#             }\n",
        "#         }\n",
        "#     },\n",
        "#     \"vector_store\": {                                                                                 # Vector store settings                             \n",
        "#             \"type\": \"chromadb\",\n",
        "#             \"persist_directory\": \"./data/chroma_db\",\n",
        "#             \"collection_name\": \"demo\",\n",
        "#             \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"                               # Options: offline-hash, sentence-transformers/all-MiniLM-L6-v2\n",
        "#         },\n",
        "#     \"prompts\": {\n",
        "#             \"path\": \"prompts/default_prompt.json\"\n",
        "#         },\n",
        "#     \"chunking\": {\n",
        "#         \"chunk_size\": 512,                                                                        # (~40 sentences, ~2,000 chars)\n",
        "#         \"chunk_overlap\": 50,\n",
        "#         \"auto_adjust\": True                                                                       # Auto-adjust chunk size based on model context window              \n",
        "#     },\n",
        "#     \"agents\": {\n",
        "#         \"planner\": {                                                                              \n",
        "#             \"enabled\": False,\n",
        "#             \"provider\": \"gemini\"\n",
        "#         },\n",
        "#         \"generator\": {\n",
        "#             \"provider\": \"gemini\",\n",
        "#             \"max_retries\": 3\n",
        "#         },\n",
        "#         \"synthesis\": {\n",
        "#             \"provider\": \"deepseek\",\n",
        "#             \"context_window\": 3,                                                                  # Number of top relevant chunks to consider\n",
        "#             \"max_evidence_spans\": 3                                                               # Number of evidence spans to cite in the answer\n",
        "#         },\n",
        "#         \"evaluator\": {\n",
        "#             \"provider\": \"mistral\",\n",
        "#             \"quality_threshold\": 0.75,\n",
        "#             \"metrics\": [\"relevance\", \"clarity\", \"completeness\", \"factuality\", \"diversity\"]         # Evaluation metrics (Adjust as needed) \n",
        "#         }\n",
        "#     },\n",
        "#     \"bloom_level\": {\n",
        "#         \"enabled\": False,\n",
        "#         \"levels\": [\"remember\", \"understand\", \"apply\", \"analyze\", \"evaluate\", \"create\"]             # Bloom's taxonomy levels\n",
        "#     },\n",
        "#     \"human_review\": {\n",
        "#         \"enabled\": False,                                                                          # Enable human review step\n",
        "#         \"review_threshold\": 0.6\n",
        "#     },\n",
        "#     \"export\": {\n",
        "#         \"format\": \"json\",\n",
        "#         \"include_metadata\": True,\n",
        "#         \"output_path\": \"./data/output\"\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Save config\n",
        "# config_path = PROJECT_ROOT / 'config.json'\n",
        "# with open(config_path, 'w') as f:\n",
        "#     json.dump(config, indent=2, fp=f)\n",
        "\n",
        "# print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
        "# print(\"‚ö†Ô∏è  Remember to update API keys in config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a78987e",
      "metadata": {},
      "source": [
        "## Load the config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9550cb98",
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.config import load_config\n",
        "\n",
        "# Reload config (in case you updated API keys)\n",
        "cfg = load_config(PROJECT_ROOT / 'config.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Loading & Preprocessing\n",
        "\n",
        "The pipeline starts by ingesting raw text files and preparing them for chunking.\n",
        "\n",
        "**Key Operations**:\n",
        "- Strip Gutenberg headers/footers and metadata\n",
        "- Normalize whitespace and remove special characters\n",
        "- Extract document metadata (title, author, source)\n",
        "===================================================================================================================\n",
        "- **Component:** `pipeline.document_loaders.load_document` + `utils.text_processing.clean_text/normalize_text`\n",
        "- **Input:** Individual `.txt` docs.\n",
        "- **Output:** Cleaned text plus structured metadata ready for semantic chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stashed demo document: Rudyard Kipling___The Jungle Book.txt (271,339 characters)\n",
            "Preview of demo document:\n",
            "========================\n",
            " Mowgli's Brothers\n",
            "\n",
            " Now Rann the Kite brings home the night\n",
            " That Mang the Bat sets free--\n",
            " The herds are shut in byre and hut\n",
            " For loosed till dawn are we.\n",
            " This is the hour of pride and power,\n",
            " Talon and tush and claw.\n",
            " Oh, hear the call!--Good hunting all\n",
            " That keep the Jungle Law!\n",
            " Night-Song in the Jungle\n",
            "\n",
            "It was seven o'clock of a very warm evening in the Seeonee hills when\n",
            "Father Wolf woke up from his day's re...\n"
          ]
        }
      ],
      "source": [
        "from pipeline.document_loaders import load_document\n",
        "from utils.text_processing import clean_text, normalize_text\n",
        "\n",
        "cleaned_documents = {}\n",
        "\n",
        "for path in paper_paths:\n",
        "    raw_doc = load_document(str(path))\n",
        "    cleaned_text = normalize_text(clean_text(raw_doc[\"content\"]))\n",
        "    cleaned_documents[path.name] = {\n",
        "        \"metadata\": raw_doc[\"metadata\"],\n",
        "        \"text\": cleaned_text\n",
        "    }\n",
        "\n",
        "# Stash The Jungle Book novel for downstream demos\n",
        "demo_doc_name = \"Rudyard Kipling___The Jungle Book.txt\"\n",
        "demo_doc = cleaned_documents[demo_doc_name]\n",
        "print(f\"Stashed demo document: {demo_doc_name} ({len(demo_doc['text']):,} characters)\")\n",
        "print(\"Preview of demo document:\\n========================\\n\", demo_doc['text'][:420] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Chunking\n",
        "\n",
        "Documents are split into overlapping chunks to maintain context continuity.\n",
        "\n",
        "**Parameters**:\n",
        "- `chunk_size`: 512 tokens (~400 words) (default)\n",
        "- `overlap`: 50 tokens (prevents context loss at boundaries) (default)\n",
        "\n",
        "**Why Overlapping?** Adjacent chunks share context, enabling the retriever to capture information that spans chunk boundaries.\n",
        "\n",
        "Each chunk receives a unique ID and inherits parent document metadata.\n",
        "\n",
        "===================================================================================================================\n",
        "\n",
        "- **Component:** `pipeline.chunking.chunk_text`\n",
        "- **Input:** Cleaned text from the first book: \"G K Chesterton___The Man Who Knew Too Much.txt\" with chunk size 256 tokens (~1,024 chars or around 20-30 English sentences) and overlap 40 tokens\n",
        "- **Output:** Structured `Chunk` objects with ids, token estimates, and metadata for vector storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input doc: Rudyard Kipling___The Jungle Book.txt\n",
            "Requested chunk size / overlap: 256 / 20\n",
            "Output: 288 chunks\n",
            "First chunk sample:\n",
            "  chunk_id: f9285c6b-1bd1-4eec-9bb3-53b8ae26e2c0\n",
            "  tokens: 226\n",
            "  position: 0\n",
            "  text: Mowgli's Brothers\n",
            "\n",
            " Now Rann the Kite brings home the night\n",
            " That Mang the Bat sets free--\n",
            " The herds are shut in byre and hut\n",
            " For loosed till dawn are we.\n",
            " This is the hour of pride and power,\n",
            " Talon and tush and claw.\n",
            " Oh, hear the call!--Good hunting all\n",
            " That keep the Jungle Law!\n",
            " Night-Song in the Jungle\n",
            "\n",
            "It was seven o'clock of a very warm evening in the Seeonee hills when\n",
            "Father Wolf woke up from his day's re...\n"
          ]
        }
      ],
      "source": [
        "from pipeline.chunking import chunk_text\n",
        "\n",
        "chunk_size = cfg.chunking.chunk_size                                                          # Or load from config\n",
        "chunk_overlap = cfg.chunking.chunk_overlap\n",
        "\n",
        "demo_chunks = chunk_text(\n",
        "    demo_doc[\"text\"],                                                   # Contains Rudyard Kipling___The Jungle Book.txt\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    metadata={\"filename\": demo_doc_name}\n",
        ")\n",
        "\n",
        "print(f\"Input doc: {demo_doc_name}\")\n",
        "print(f\"Requested chunk size / overlap: {chunk_size} / {chunk_overlap}\")\n",
        "print(f\"Output: {len(demo_chunks)} chunks\")\n",
        "\n",
        "first_chunk = demo_chunks[0]                                            # first_chunk of the chunk of Rudyard Kipling___The Jungle Book\n",
        "print(\"First chunk sample:\")\n",
        "print(f\"  chunk_id: {first_chunk.chunk_id}\")\n",
        "print(f\"  tokens: {first_chunk.tokens}\")\n",
        "print(f\"  position: {first_chunk.position}\")\n",
        "print(f\"  text: {first_chunk.text[:420]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Store: ChromaDB + Embeddings\n",
        "\n",
        "**Purpose**: Enable semantic search over document chunks.\n",
        "\n",
        "**Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`\n",
        "- Fast, open-source, 384-dimensional embeddings\n",
        "- Trained on semantic similarity tasks\n",
        "\n",
        "**Storage**: Persistent ChromaDB collection\n",
        "- Chunks indexed by semantic vectors\n",
        "- Metadata stored alongside embeddings for filtering\n",
        "\n",
        "The retriever queries this store to find relevant context for answer generation.\n",
        "\n",
        "===================================================================================================================\n",
        "- **Component:** `core.vector_store.VectorStoreManager` + `pipeline.ingestion.IngestionPipeline`\n",
        "- **Input:** Four cleaned documents with offline hash embeddings (chunk size 256, overlap 40)\n",
        "- **Output:** Persisted Chroma collection plus an ingestion report per file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kaizu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input file: G K Chesterton___The Man Who Knew Too Much.txt\n",
            "Output chunk ids stored: 348\n",
            "Input file: Herbert Spencer___Essays on Education and Kindred Subjects.txt\n",
            "Output chunk ids stored: 933\n",
            "Input file: Jack London___The Faith of Men.txt\n",
            "Output chunk ids stored: 274\n",
            "Input file: Rudyard Kipling___The Jungle Book.txt\n",
            "Output chunk ids stored: 288\n",
            "Vector store now holds: 1843 chunks\n",
            "Ingestion summary: {'G K Chesterton___The Man Who Knew Too Much.txt': 348, 'Herbert Spencer___Essays on Education and Kindred Subjects.txt': 933, 'Jack London___The Faith of Men.txt': 274, 'Rudyard Kipling___The Jungle Book.txt': 288}\n"
          ]
        }
      ],
      "source": [
        "from core.vector_store import VectorStoreManager\n",
        "from pipeline.ingestion import IngestionPipeline\n",
        "\n",
        "vector_dir = PROJECT_ROOT / \"data\" / \"output\" / \"demo_chroma\"               # Contains vector database\n",
        "if vector_dir.exists():\n",
        "    shutil.rmtree(vector_dir)\n",
        "vector_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if False:                                                                    # Reset vector store if needed\n",
        "    # Delete existing ChromaDB (if any)\n",
        "    import shutil\n",
        "    if vector_dir.exists():\n",
        "        shutil.rmtree(vector_dir)\n",
        "    # Reset vector store\n",
        "    vs_manager.reset_collection()\n",
        "    print(\"‚úÖ Vector store reset and ready for re-ingestion.\")\n",
        "\n",
        "vs_manager = VectorStoreManager(                                           # Create vector store instance\n",
        "    persist_directory=str(vector_dir),\n",
        "    collection_name=\"demo_novels\",\n",
        "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",              # Options: sentence-transformers/all-MiniLM-L6-v2, cohere, offline-hash, or any huggingface embedding model\n",
        "    embedding_dimension=384                                                # all-MiniLM-L6-v2 uses 384 dimensions, offline-hash is flexible\n",
        ")\n",
        "\n",
        "ingestion = IngestionPipeline(\n",
        "    vector_store_manager=vs_manager,\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "ingestion_summary = {}\n",
        "for path in paper_paths:                                                   # Ingest all selected raw text files \n",
        "    doc_ids = ingestion.ingest_document(str(path))\n",
        "    ingestion_summary[path.name] = len(doc_ids)\n",
        "    print(f\"Input file: {path.name}\")\n",
        "    print(f\"Output chunk ids stored: {len(doc_ids)}\")\n",
        "\n",
        "print(\"Vector store now holds:\", vs_manager.get_collection_count(), \"chunks\")\n",
        "print(\"Ingestion summary:\", ingestion_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk Analysis Tool\n",
        "- **Component:** `tools.chunk_tool.ChunkAnalysisTool`\n",
        "- **Input:** First chunk text from the lead book: \"G K Chesterton___The Man Who Knew Too Much\"\n",
        "- **Output:** Sentence/entity stats plus suggested question types for the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input chunk preview:\n",
            "Mowgli's Brothers\n",
            "\n",
            " Now Rann the Kite brings home the night\n",
            " That Mang the Bat sets free--\n",
            " The herds are shut in byre and hut\n",
            " For loosed till dawn are we.\n",
            " This is the hour of pride and power,\n",
            " Talon and tush and claw....\n",
            "Analysis output:\n",
            "  sentence_count: 7\n",
            "  word_count: 171\n",
            "  entities: ['It', 'Mother', 'Now', 'Father', 'Wolves', 'For', 'Wolf', 'Jungle', 'The', 'Bat']\n",
            "  has_numbers: False\n",
            "  number_count: 0\n",
            "  potential_topics: 27\n",
            "  question_potential: True\n",
            "=======================================================================================\n",
            "Suggested question types: ['factual', 'conceptual', 'analytical']\n"
          ]
        }
      ],
      "source": [
        "from tools.chunk_tool import ChunkAnalysisTool\n",
        "\n",
        "chunk_analyzer = ChunkAnalysisTool()\n",
        "analysis = chunk_analyzer.analyze_chunk(first_chunk.text)\n",
        "suggestions = chunk_analyzer.suggest_question_types(analysis)           # Suggest possible question types (Rule Based) out of five based on analysis based on entities, sentence structure, complexity, and length\n",
        "\n",
        "print(\"Input chunk preview:\")\n",
        "print(first_chunk.text[:220] + \"...\")\n",
        "\n",
        "print(\"Analysis output:\")\n",
        "for key, value in analysis.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"=============================\"*3)\n",
        "print(\"Suggested question types:\", suggestions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieval Tool\n",
        "- **Component:** `tools.retrieval_tool.RetrievalTool`\n",
        "- **Input:** Query to the vector database\n",
        "- **Output:** Formatted context snippets pulled from the Chroma store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input query: How many holluschickies were in Kotick's army?\n",
            "=============================\n",
            "Retrieved context:\n",
            "[Chunk 1]\n",
            "Source: Rudyard Kipling___The Jungle Book.txt\n",
            "Position: 156\n",
            "Content: vastoshnah, leaving the gulls to scream. There he\n",
            "found that no one sympathized with him in his little attempt to discover\n",
            "a quiet place for the seals. They told him that men had always driven\n",
            "the holluschickie--it was part of the day's work--and that if he did not\n",
            "like to see ugly things he should not have gone to the killing grounds.\n",
            "But none of the other seals had seen the killing, and that made the\n",
            "difference between him and his friends. Besides, Kotick was a white\n",
            "seal.\n",
            "\n",
            "\"What you must do,\" said old Sea Catch, after he had heard his son's\n",
            "adventures, \"is to grow up and be a big seal like your father, and have\n",
            "a nursery on the beach, and then they will leave you alone. In another\n",
            "five years you ought to be able to fight for yourself.\" Even gentle\n",
            "Matkah, his mother, said: \"You will never be able to stop the killing.\n",
            "Go and play in the sea, Kotick.\" And Kotick went off and danced the\n",
            "Fire-dance with a very heavy little heart.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tools.retrieval_tool import RetrievalTool\n",
        "\n",
        "retrieval_tool = RetrievalTool(vs_manager)\n",
        "retrieval_query = \"How many holluschickies were in Kotick's army?\"                                  # The answer is about ten thousands, which is exists in the top 1 context: 1024 tokens\n",
        "retrieved_context = retrieval_tool.retrieve_context(retrieval_query, k=1)                           # Retrieve top-1 relevant chunk\n",
        "\n",
        "print(\"Input query:\", retrieval_query)\n",
        "print(\"=============================\")\n",
        "print(\"Retrieved context:\")\n",
        "print(retrieved_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859300ff",
      "metadata": {},
      "source": [
        "## Test and initialize a LLM following config\n",
        "\n",
        "BSSC-QA supports multiple LLM providers through a unified interface.\n",
        "\n",
        "\n",
        "Each agent can use a different provider, enabling cost/performance optimization per task.\n",
        "\n",
        "*This cell initializes the LLM provider manager from config.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24a5f471",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kaizu/Projects/test/BSSC_QA/bssc_qa/src/core/llm_factory.py:24: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import ChatOpenAI``.\n",
            "  return ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "from core.llm_factory import create_llm\n",
        "from core.config import load_config\n",
        "\n",
        "# Reload config (in case you updated API keys)\n",
        "cfg = load_config(PROJECT_ROOT / 'config.json')\n",
        "\n",
        "# Get default provider config\n",
        "provider_name = cfg.llm.default_provider\n",
        "provider_cfg = cfg.llm.providers[provider_name]\n",
        "\n",
        "# Create LLM\n",
        "llm = create_llm(\n",
        "    provider=provider_name,\n",
        "    api_key=provider_cfg.api_key,\n",
        "    model=provider_cfg.model,\n",
        "    temperature=provider_cfg.temperature\n",
        ")\n",
        "\n",
        "# # Test with simple prompt\n",
        "# response = llm.invoke(\"Say 'Hello from BSSC_QA!' and tell a pun about academic research.\")\n",
        "# print(f\"‚úÖ LLM ({provider_name}) Response: {response.content}\\n-_-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Generator Agent\n",
        "**Role**: Generate questions from document chunks using LLM prompting.\n",
        "\n",
        "**Process**:\n",
        "1. Receives a text chunk\n",
        "2. Analyzes content for key concepts\n",
        "3. Generates N diverse questions covering chunk topics\n",
        "\n",
        "**Output**: List of questions with metadata (chunk_id, difficulty level).\n",
        "\n",
        "===================================================================================================================\n",
        "\n",
        "**System Prompt:**\n",
        "\n",
        "    \n",
        "```python\n",
        "    \"\"\"\n",
        "    You are an expert question generator. Your task is to create high-quality, \n",
        "    diverse questions from given text content.\n",
        "\n",
        "    Guidelines:\n",
        "    1. Questions should be clear, specific, and answerable from the content\n",
        "    2. Vary question types: factual, conceptual, analytical\n",
        "    3. Ask about key concepts, entities, and relationships\n",
        "    4. Ensure questions test understanding, not just recall\n",
        "    5. Each question should be complete and grammatically correct\n",
        "\n",
        "    Output format for each question:\n",
        "    {\n",
        "    \"question\": \"Your question here?\",\n",
        "    \"type\": \"factual|conceptual|analytical\",\n",
        "    \"rationale\": \"Why this question is valuable\"\n",
        "    }\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**User Prompt:**\n",
        "\n",
        "```python\n",
        "    \"\"\"\n",
        "    Generate {count} diverse, high-quality questions from this content:\n",
        "\n",
        "    {chunk_text}\n",
        "\n",
        "    Provide exactly {count} questions in the specified JSON format.\n",
        "    \"\"\"\n",
        "```\n",
        "===================================================================================================================\n",
        "- **Component:** `agents.generator_agent.GeneratorAgent`\n",
        "- **Input:** First chunk of text (‚âà200 chars shown) with a request for 2 questions\n",
        "- **Output:** Structured questions (text, type, rationale) ready for synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:LLM ChatOpenAI does not support tool binding; falling back to direct prompting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input chunk preview:\n",
            "Mowgli's Brothers\n",
            "\n",
            " Now Rann the Kite brings home the night\n",
            " That Mang the Bat sets free--\n",
            " The herds are shut in byre and hut\n",
            " For loosed till dawn are we.\n",
            " This is the hour of pride and power,\n",
            " Talo...\n",
            "Generated questions:\n",
            "- (factual) What time of day does Father Wolf wake up from his rest in the Seeonee hills?\n",
            "  rationale: This question tests basic comprehension of the temporal setting and establishes the story's opening context\n",
            "- (conceptual) How does the narrator describe the relationship between Mother Wolf and her cubs in the cave?\n",
            "  rationale: This question examines understanding of family dynamics and the protective maternal imagery in the text\n"
          ]
        }
      ],
      "source": [
        "from agents.generator_agent import GeneratorAgent\n",
        "\n",
        "generator = GeneratorAgent(\n",
        "    llm=llm,                                                      \n",
        "    retrieval_tool=retrieval_tool,\n",
        "    chunk_analysis_tool=chunk_analyzer\n",
        ")\n",
        "\n",
        "questions = generator.generate_questions(first_chunk.text, count=2)                             # Generate 2 questions from the first chunk\n",
        "\n",
        "print(\"Input chunk preview:\")\n",
        "print(first_chunk.text[:200] + \"...\")\n",
        "\n",
        "print(\"Generated questions:\")\n",
        "for q in questions:\n",
        "    print(f\"- ({q['question_type']}) {q['question']}\")\n",
        "    print(f\"  rationale: {q['rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer Synthesis Agent\n",
        "\n",
        "**Role**: Generate grounded answers using retrieved evidence.\n",
        "\n",
        "**Workflow**:\n",
        "1. Query vector store with question\n",
        "2. Retrieve top-K relevant chunks (context window)\n",
        "3. Generate answer citing specific evidence spans\n",
        "\n",
        "**Key Feature**: Answer provenance tracking\n",
        "- Each answer includes 1-3 evidence spans\n",
        "- Spans reference source chunks for verification\n",
        "\n",
        "*Defaults to DeepSeek for its strong context reasoning.*\n",
        "\n",
        "===================================================================================================================\n",
        "\n",
        "**System Prompt:**\n",
        "```python\n",
        "    \"\"\"\n",
        "    You are an expert answer synthesizer. Your task is to create accurate, \n",
        "    comprehensive answers based on provided evidence. Make sure the answers are short and concise.\n",
        "\n",
        "    Guidelines:\n",
        "    1. Base answers strictly on the evidence provided\n",
        "    2. Be clear, concise, and well-structured\n",
        "    3. Include relevant details and context\n",
        "    4. Maintain factual accuracy\n",
        "    5. Match answer complexity to question complexity\n",
        "\n",
        "    Your answer should:\n",
        "    - Directly address the question\n",
        "    - Use evidence to support claims\n",
        "    - Be complete but not unnecessarily verbose\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**User Prompt:**\n",
        "\n",
        "```python\n",
        "    \"\"\"\n",
        "    Question: {question}\n",
        "    Question Type: {question_type}\n",
        "\n",
        "    Evidence:\n",
        "    {evidence_text}\n",
        "\n",
        "    Based on the evidence above, provide a clear and accurate answer to the question.\n",
        "    \"\"\"\n",
        "```\n",
        "===================================================================================================================\n",
        "\n",
        "- **Component:** `agents.synthesis_agent.SynthesisAgent`\n",
        "- **Input:** First generated question plus vector-store evidence (k=2)\n",
        "- **Output:** Answer text with captured evidence spans inside the QA record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Synthesis prompt: \n",
            "Question: What time of day does Father Wolf wake up from his rest in the Seeonee hills?\n",
            "Question Type: factual\n",
            "\n",
            "Evidence:\n",
            "[Evidence 1]\n",
            "Mowgli's Brothers\n",
            "\n",
            " Now Rann the Kite brings home the night\n",
            " That Mang the Bat sets free--\n",
            " The herds are shut in byre and hut\n",
            " For loosed till dawn are we.\n",
            " This is the hour of pride and power,\n",
            " Talon and tush and claw.\n",
            " Oh, hear the call!--Good hunting all\n",
            " That keep the Jungle Law!\n",
            " Night-Song in the Jungle\n",
            "\n",
            "It was seven o'clock of a very warm evening in the Seeonee hills\n",
            "Input question: What time of day does Father Wolf wake up from his rest in the Seeonee hills?\n",
            "Synthesized answer: Father Wolf wakes up from his day's rest at seven o'clock in the evening in the Seeonee hills.\n",
            "\n",
            "**Supporting Evidence:**  \n",
            "From \"Mowgli's Brothers\": \"It was seven o'clock of a very warm evening in the Seeonee hills when Father Wolf woke up from his day's rest.\"\n",
            "Evidence count: 2\n"
          ]
        }
      ],
      "source": [
        "from agents.synthesis_agent import SynthesisAgent\n",
        "\n",
        "synthesizer = SynthesisAgent(\n",
        "    llm=llm,\n",
        "    vector_store_manager=vs_manager,\n",
        "    max_evidence_spans=2\n",
        ")\n",
        "\n",
        "demo_question = questions[0]                                                # Take the first generated question\n",
        "qa_pair = synthesizer.synthesize_answer(\n",
        "    demo_question[\"question\"],\n",
        "    demo_question[\"question_type\"]\n",
        ")\n",
        "\n",
        "print(\"Input question:\", demo_question[\"question\"])\n",
        "print(\"Synthesized answer:\", qa_pair[\"answer\"])\n",
        "print(\"Evidence count:\", len(qa_pair[\"evidence_spans\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e7dd0ab5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'qa_id': '90f44a83-a872-438e-a9bc-7bfaf29994b3',\n",
              " 'question': 'What time of day does Father Wolf wake up from his rest in the Seeonee hills?',\n",
              " 'answer': 'Father Wolf wakes up from his day\\'s rest at seven o\\'clock in the evening in the Seeonee hills.\\n\\n**Supporting Evidence:**  \\nFrom \"Mowgli\\'s Brothers\": \"It was seven o\\'clock of a very warm evening in the Seeonee hills when Father Wolf woke up from his day\\'s rest.\"',\n",
              " 'evidence_spans': ['Mowgli\\'s Brothers\\n\\n Now Rann the Kite brings home the night\\n That Mang the Bat sets free--\\n The herds are shut in byre and hut\\n For loosed till dawn are we.\\n This is the hour of pride and power,\\n Talon and tush and claw.\\n Oh, hear the call!--Good hunting all\\n That keep the Jungle Law!\\n Night-Song in the Jungle\\n\\nIt was seven o\\'clock of a very warm evening in the Seeonee hills when\\nFather Wolf woke up from his day\\'s rest, scratched himself, yawned, and\\nspread out his paws one after the other to get rid of the sleepy feeling\\nin their tips. Mother Wolf lay with her big gray nose dropped across her\\nfour tumbling, squealing cubs, and the moon shone into the mouth of the\\ncave where they all lived. \"Augrh!\" said Father Wolf. \"It is time to\\nhunt again.\" He was going to spring down hill when a little shadow with\\na bushy tail crossed the threshold and whined: \"Good luck go with you, O\\nChief of the Wolves.',\n",
              "  'e, and drew\\nbreath, and looked down the valley. The cubs were out, but Mother\\nWolf, at the back of the cave, knew by his breathing that something was\\ntroubling her frog.\\n\\n\"What is it, Son?\" she said.\\n\\n\"Some bat\\'s chatter of Shere Khan,\" he called back. \"I hunt among the\\nplowed fields tonight,\" and he plunged downward through the bushes, to\\nthe stream at the bottom of the valley. There he checked, for he heard\\nthe yell of the Pack hunting, heard the bellow of a hunted Sambhur,\\nand the snort as the buck turned at bay. Then there were wicked, bitter\\nhowls from the young wolves: \"Akela! Akela! Let the Lone Wolf show his\\nstrength. Room for the leader of the Pack! Spring, Akela!\"\\n\\nThe Lone Wolf must have sprung and missed his hold, for Mowgli heard the\\nsnap of his teeth and then a yelp as the Sambhur knocked him over with\\nhis forefoot.\\n\\nHe did not wait for anything more, but dashed on; and the yells grew\\nfainter behind him as he ran into the croplands where the villagers\\nlived.'],\n",
              " 'chunk_ids': ['60dd52e5-f4fe-48c3-9b27-798e6058dc08',\n",
              "  '48742e34-6b7f-4535-8cb9-5d4e1c62039f'],\n",
              " 'question_type': 'factual'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluator Agent\n",
        "\n",
        "**Role**: Score QA pairs across multiple dimensions.\n",
        "\n",
        "**Evaluation Metrics**:\n",
        "- **Relevance**: Question-answer alignment\n",
        "- **Clarity**: Language quality and coherence\n",
        "- **Completeness**: Answer thoroughness\n",
        "- **Factuality**: Grounding in evidence\n",
        "- **Format**: Structural correctness\n",
        "\n",
        "**Scoring**: 0-1 scale per metric, aggregated to overall score.\n",
        "\n",
        "**Quality Threshold**: 0.75 (configurable)\n",
        "- QA pairs below threshold are flagged\n",
        "\n",
        "*Uses Mistral for consistent evaluation.*\n",
        "\n",
        "===================================================================================================================\n",
        "\n",
        "**System Promt**\n",
        "```python\n",
        "\n",
        "    \"\"\"\n",
        "    You are a quality evaluation expert. Your task is to assess QA pairs \n",
        "    across multiple dimensions and provide detailed scores.\n",
        "\n",
        "    Evaluation Criteria:\n",
        "    1. Relevance (0-1): Does the answer address the question?\n",
        "    2. Clarity (0-1): Are both Q&A clear and unambiguous?\n",
        "    3. Completeness (0-1): Is the answer comprehensive?\n",
        "    4. Factuality (0-1): Is the answer accurate based on evidence?\n",
        "\n",
        "    Provide scores for each criterion and identify any issues.\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**User Promt:**\n",
        "\n",
        "```python\n",
        "    \"\"\"\n",
        "    Evaluate this QA pair:\n",
        "\n",
        "    Question: {qa['question']}\n",
        "    Answer: {qa['answer']}\n",
        "\n",
        "    Evidence:\n",
        "    {evidence_text[:-1]}\n",
        "\n",
        "    Provide scores (0.0 to 1.0) for:\n",
        "    - Relevance\n",
        "    - Clarity\n",
        "    - Completeness\n",
        "    - Factuality\n",
        "\n",
        "    Format: \n",
        "    relevance: X.X\n",
        "    clarity: X.X\n",
        "    completeness: X.X\n",
        "    factuality: X.X\n",
        "    \"\"\"\n",
        "```\n",
        "===================================================================================================================\n",
        "\n",
        "- **Component:** `agents.evaluator_agent.EvaluatorAgent` + `tools.validation_tool.ValidationTool`\n",
        "- **Input:** QA pair emitted by the synthesizer\n",
        "- **Output:** Combined rule-based + LLM-style quality scores and pass/fail signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input QA ID: a6e14956-407e-455d-b39a-e3bc0927fc9d\n",
            "Scores: {'length': 1.0, 'answer_length': 1.0, 'format': 1.0, 'relevance': 1.0, 'completeness': 1.0, 'clarity': 1.0, 'factuality': 1.0}\n",
            "Overall score: 1.0\n",
            "Passed quality bar: True\n",
            "Flags: []\n"
          ]
        }
      ],
      "source": [
        "from tools.validation_tool import ValidationTool\n",
        "from agents.evaluator_agent import EvaluatorAgent\n",
        "\n",
        "validator = ValidationTool(quality_threshold=0.6)\n",
        "evaluator = EvaluatorAgent(\n",
        "    llm=llm,\n",
        "    validation_tool=validator,\n",
        "    quality_threshold=0.7\n",
        ")\n",
        "\n",
        "evaluation = evaluator.evaluate_qa(qa_pair)                         # Evaluate the selected synthesized QA pair\n",
        "\n",
        "print(\"Input QA ID:\", qa_pair[\"qa_id\"])\n",
        "print(\"Scores:\", evaluation[\"scores\"])\n",
        "print(\"Overall score:\", round(evaluation[\"overall_score\"], 2))\n",
        "print(\"Passed quality bar:\", evaluation[\"passed\"])\n",
        "print(\"Flags:\", evaluation[\"flags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline Orchestrator\n",
        "\n",
        "The orchestrator coordinates all agents in sequence:\n",
        "\n",
        "**Pipeline Flow**:\n",
        "1. Sample N random chunks from vector store\n",
        "2. **Generator**: Create M questions per chunk\n",
        "3. **Synthesis**: Generate answers with evidence retrieval\n",
        "4. **Evaluator**: Score each QA pair\n",
        "\n",
        "**Output Statistics**:\n",
        "- Total QA pairs generated\n",
        "- Pass/fail counts (based on quality threshold)\n",
        "- Aggregate metrics (diversity, difficulty distribution)\n",
        "\n",
        "**This cell**: Generate 6 QA pairs (3 chunks √ó 2 questions) and display summary statistics.\n",
        "\n",
        "===================================================================================================================\n",
        "\n",
        "- **Component:** `pipeline.orchestrator.QAPipelineOrchestrator`\n",
        "- **Input:** 3 sampled chunks from the vector store with 1 question per chunk\n",
        "- **Output:** Aggregated QA dataset plus evaluation statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving 3 chunks...\n",
            "\n",
            "Processing 3 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating QA:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "pier if we hauled out at Otter\n",
            "Island instead of this crowded place,\" said Matkah.\n",
            "\n",
            "\"Bah! Only the holluschickie go to Otter Island. If we went there they\n",
            "would say we were afraid. We must preserve appearances, my dear.\"\n",
            "\n",
            "Sea Catch sunk his he\n",
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "come with you to\n",
            "your island--if there is such a place.\"\n",
            "\n",
            "\"Hear you, fat pigs of the sea. Who comes with me to the Sea Cow's\n",
            "tunnel? Answer, or I shall teach you again,\" roared Kotick.\n",
            "\n",
            "There was a murmur like the ripple of the tide all up and\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating QA:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:28<00:57, 28.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "d French\n",
            "romances, but a good many wouldn't think about it at all. They would\n",
            "just swallow the skepticism because it was skepticism. Modern\n",
            "intelligence won't accept anything on authority. But it will accept\n",
            "anything without authority. That's \n",
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "riors. Not that he delighted in the work, but that it was the one\n",
            "thing that prevented him from going mad.\n",
            "\n",
            "The first year he wished he was dead. The second year he cursed God. The\n",
            "third year he was divided between the two emotions, and in the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating QA:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [01:03<00:32, 32.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "low voice:\n",
            "\n",
            "\"I suppose it's all right about air?\"\n",
            "\n",
            "\"Oh, yes,\" replied the other aloud; \"there's a fireplace and a\n",
            "chimney in the office just by the door.\"\n",
            "\n",
            "A bound and the noise of a falling chair told them that the\n",
            "irrepressible rising genera\n",
            "üßæ Retrieved these evidences: \n",
            "[Evidence 1]\n",
            "retences,\" he said, with a smile. \"I\n",
            "hardly even know what an archaeologist is, except that a rather\n",
            "rusty remnant of Greek suggests that he is a man who studies old\n",
            "things.\"\n",
            "\n",
            "\"Yes,\" replied Haddow, grimly. \"An archaeologist is a man who\n",
            "studi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:30<00:00, 30.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline output summary:\n",
            "  total_chunks: 3\n",
            "  total_questions_attempted: 6\n",
            "  total_qa_pairs: 6\n",
            "  passed_qa_pairs: 6\n",
            "  pass_rate: 100.00%\n",
            "  sample QA IDs: ['5e15c38e-7e4d-40b1-8722-7031bc19422c', 'd3bcc6f6-5f48-4812-974f-46ab1cf164a6']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pipeline.orchestrator import QAPipelineOrchestrator\n",
        "\n",
        "orchestrator = QAPipelineOrchestrator(\n",
        "    generator_agent=generator,\n",
        "    synthesis_agent=synthesizer,\n",
        "    evaluator_agent=evaluator,\n",
        "    vector_store_manager=vs_manager,\n",
        "    config={}\n",
        ")\n",
        "\n",
        "results = orchestrator.generate_qa_from_chunks(\n",
        "    num_chunks=3,\n",
        "    questions_per_chunk=2\n",
        ")\n",
        "\n",
        "print(\"Pipeline output summary:\")\n",
        "print(f\"  total_chunks: {results['total_chunks']}\")\n",
        "print(f\"  total_questions_attempted: {results['total_questions_attempted']}\")\n",
        "print(f\"  total_qa_pairs: {results['total_qa_pairs']}\")\n",
        "print(f\"  passed_qa_pairs: {results['passed_qa_pairs']}\")\n",
        "print(f\"  pass_rate: {results['statistics'].get('pass_rate', 0):.2%}\")\n",
        "print(f\"  sample QA IDs: {[qa['qa_id'] for qa in results['qa_pairs'][:2]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c54375f",
      "metadata": {},
      "source": [
        "## Examining Generated QA Pairs\n",
        "\n",
        "Each QA pair includes:\n",
        "- **Question**: Generated query\n",
        "- **Answer**: Synthesized response with evidence\n",
        "- **Evidence Spans**: Source text citations (2-3 per answer)\n",
        "- **Evaluation Scores**: Per-metric breakdown\n",
        "- **Overall Score**: Weighted average (0-1)\n",
        "- **Pass Status**: Whether it meets quality threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "38018948",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============================\n",
            "QA ID: 5e15c38e-7e4d-40b1-8722-7031bc19422c\n",
            "Question: What natural features prevent ships from approaching within six miles of the beach?\n",
            "Answer: According to the evidence, the natural features that prevent ships from approaching within six miles of the beach are:\n",
            "\n",
            "- A line of bars, shoals, and rocks running northward out to sea\n",
            "- These shoals would \"knock a ship to splinters\" if attempted to navigate\n",
            "\n",
            "These features create a protective barrier that keeps ships at a safe distance from the coastline.\n",
            "Evidence spans: 2\n",
            "Evaluation scores: {'length': 1.0, 'answer_length': 1.0, 'format': 1.0, 'relevance': 1.0, 'completeness': 0.8, 'clarity': 1.0, 'factuality': 1.0}\n",
            "Overall score: 0.97\n",
            "Passed quality bar: True\n",
            "\n",
            "=============================\n",
            "QA ID: d3bcc6f6-5f48-4812-974f-46ab1cf164a6\n",
            "Question: Why does Kotick conclude that this location is safer than Novastoshnah?\n",
            "Answer: Based on the evidence provided, Kotick concludes that the location beyond Sea Cow's Tunnel is safer than Novastoshnah because:\n",
            "\n",
            "1. **It is free from human hunters** ‚Äì Unlike other islands Kotick explored, which showed signs of seals being killed by humans (Evidence 2), the new location has no evidence of human presence or hunting.\n",
            "\n",
            "2. **It proves successful and peaceful** ‚Äì After leading seals there, they return with positive reports, causing more seals to abandon Novastoshnah the following spring (Evidence 1). This indicates the new beaches provide a secure habitat without the threats present at Novastoshnah, where whalers were active (Evidence 2).\n",
            "Evidence spans: 2\n",
            "Evaluation scores: {'length': 1.0, 'answer_length': 1.0, 'format': 1.0, 'relevance': 1.0, 'completeness': 1.0, 'clarity': 1.0, 'factuality': 1.0}\n",
            "Overall score: 1.0\n",
            "Passed quality bar: True\n"
          ]
        }
      ],
      "source": [
        "# Print sample QA pairs\n",
        "for qa in results['qa_pairs'][:2]:\n",
        "    print(\"\\n=============================\")\n",
        "    print(f\"QA ID: {qa['qa_id']}\")\n",
        "    print(f\"Question: {qa['question']}\")\n",
        "    print(f\"Answer: {qa['answer']}\")\n",
        "    print(f\"Evidence spans: {len(qa['evidence_spans'])}\")\n",
        "    print(f\"Evaluation scores: {qa['scores']}\")\n",
        "    print(f\"Overall score: {round(qa['overall_score'], 2)}\")\n",
        "    print(f\"Passed quality bar: {qa['passed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5e6f73",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2930fe4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1c3c4f4b",
      "metadata": {},
      "source": [
        "## Experiment: Generating Concise Answers\n",
        "\n",
        "**Goal**: Reduce answer verbosity while maintaining factual accuracy.\n",
        "\n",
        "**Config Changes**:\n",
        "- Reduced context window (3 ‚Üí 2 chunks) to limit input length\n",
        "- Lower max evidence spans (3 ‚Üí 2) to reduce citation overhead\n",
        "- Adjusted evaluation metrics (Conciseness) to prioritize brevity\n",
        "\n",
        "**Prompt Strategy** Read the prompts for shorter answers: prompts/short_prompt.json (Customize your own based on your data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e66416",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration saved to: /home/kaizu/Projects/test/BSSC_QA/config.json\n",
            "‚ö†Ô∏è  Remember to update API keys in config.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Default configuration\n",
        "config = {\n",
        "    \"llm\": {                                                                                    # LLM provider settings (You can add or remove providers)\n",
        "        \"default_provider\": \"gemini\",\n",
        "        \"providers\": {\n",
        "            \"gemini\": {                                       \n",
        "                \"api_key\": \"your_api_key_here\",\n",
        "                \"model\": \"gemini-2.5-flash\",\n",
        "                \"temperature\": 0.2                                                              # Example: lower temperature for more focused answers\n",
        "            },\n",
        "            \"deepseek\": {\n",
        "                \"api_key\": \"your_api_key_here\",\n",
        "                \"model\": \"deepseek-chat\",\n",
        "                \"temperature\": 0.2\n",
        "            },\n",
        "            \"mistral\": {\n",
        "                \"api_key\": \"your_api_key_here\",\n",
        "                \"model\": \"mistral-large-latest\",\n",
        "                \"temperature\": 0.3\n",
        "            },\n",
        "            \"huggingface\": {\n",
        "                \"api_key\": \"your_api_key_here\",\n",
        "                \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "                \"temperature\": 0.5\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {                                                                                 # Vector store settings                             \n",
        "            \"type\": \"chromadb\",\n",
        "            \"persist_directory\": \"./data/chroma_db\",\n",
        "            \"collection_name\": \"demo\",\n",
        "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"                               # Options: offline-hash, sentence-transformers/all-MiniLM-L6-v2\n",
        "        },\n",
        "    \"prompts\": {\n",
        "            \"path\": \"prompts/short_prompt.json\"                                                   # Change to short prompt\n",
        "        },\n",
        "    \"chunking\": {\n",
        "        \"chunk_size\": 256,                                                                        # (~30 sentences, ~2,000 chars)\n",
        "        \"chunk_overlap\": 20,\n",
        "        \"auto_adjust\": True                                                                       # Auto-adjust chunk size based on model context window              \n",
        "    },\n",
        "    \"agents\": {\n",
        "        \"planner\": {                                                                              \n",
        "            \"enabled\": False,\n",
        "            \"provider\": \"gemini\"\n",
        "        },\n",
        "        \"generator\": {\n",
        "            \"provider\": \"gemini\",\n",
        "            \"max_retries\": 3\n",
        "        },\n",
        "        \"synthesis\": {\n",
        "            \"provider\": \"deepseek\",\n",
        "            \"context_window\": 2,                                                                  # Number of top relevant chunks to consider\n",
        "            \"max_evidence_spans\": 3                                                               # Number of evidence spans to cite in the answer\n",
        "        },\n",
        "        \"evaluator\": {\n",
        "            \"provider\": \"mistral\",\n",
        "            \"quality_threshold\": 0.75,\n",
        "            \"metrics\": [\"relevance\", \"clarity\", \"completeness\", \"factuality\", \"diversity\"]         # Evaluation metrics (Adjust as needed) \n",
        "        }\n",
        "    },\n",
        "    \"bloom_level\": {\n",
        "        \"enabled\": False,\n",
        "        \"levels\": [\"remember\", \"understand\", \"apply\", \"analyze\", \"evaluate\", \"create\"]             # Bloom's taxonomy levels\n",
        "    },\n",
        "    \"human_review\": {\n",
        "        \"enabled\": False,                                                                          # Enable human review step\n",
        "        \"review_threshold\": 0.6\n",
        "    },\n",
        "    \"export\": {\n",
        "        \"format\": \"json\",\n",
        "        \"include_metadata\": True,\n",
        "        \"output_path\": \"./data/output\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "config_path = PROJECT_ROOT / 'config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, indent=2, fp=f)\n",
        "\n",
        "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
        "print(\"‚ö†Ô∏è  Remember to update API keys in config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37837ca",
      "metadata": {},
      "source": [
        "## Customize Prompt Templates\n",
        "\n",
        "|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| *Just an example. **prompts/short_prompt.json** contains different prompts. Edit or create a new prompt set depending on the **dataset**, **llm**, and the returned llm responds* |\n",
        "|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "\n",
        "\n",
        "**Question Generator Prompt:** \n",
        "\n",
        "```python\n",
        "    \"\"\"\n",
        "    Generate a clear, specific question from this text chunk.\n",
        "\n",
        "    TEXT CHUNK:\n",
        "    {chunk_text}\n",
        "\n",
        "    Requirements:\n",
        "    - Create ONE question that tests comprehension\n",
        "    - Focus on key facts or concepts\n",
        "    - Avoid yes/no questions\n",
        "    - Keep question concise (1 sentence max)\n",
        "\n",
        "    Question:\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**Answer Synthesis Prompt:** \n",
        "\n",
        "```python\n",
        "    \"\"\"\n",
        "    Answer the question using ONLY the provided evidence. Be concise.\n",
        "\n",
        "    QUESTION: {question}\n",
        "    Question Type: {question_type}\n",
        "\n",
        "    EVIDENCE:\n",
        "    {evidence_text}\n",
        "\n",
        "    Requirements:\n",
        "    - Answer in a few words. One or two words preferred.\n",
        "    - Cite specific evidence using [Evidence N] format\n",
        "    - Direct and factual\n",
        "\n",
        "    Answer:\n",
        "```\n",
        "\n",
        "**Evaluator Prompt:** \n",
        "```python\n",
        "    \"\"\"\n",
        "    Strictly evaluate this QA pair on a 0-1 scale for each metric:\n",
        "\n",
        "    Question: {qa['question']}\n",
        "    Answer: {qa['answer']}\n",
        "\n",
        "    Evidence:\n",
        "    {evidence_text[:-1]}\n",
        "\n",
        "    Metrics:\n",
        "    1. Relevance: Does answer address the question?\n",
        "    2. Clarity: Is answer clear and well-written?\n",
        "    3. Completeness: Are key points covered?\n",
        "    4. Factuality: Is answer grounded in evidence?\n",
        "    5. Conciseness: Is answer appropriately brief and within one or two words?\n",
        "\n",
        "    Format: \n",
        "    relevance: X.X\n",
        "    clarity: X.X\n",
        "    completeness: X.X\n",
        "    factuality: X.X\n",
        "    \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7af755",
      "metadata": {},
      "source": [
        "## Rerun with updated Config\n",
        "\n",
        "After saving the new config rerun and comment the previous config.\n",
        "\n",
        "\n",
        "*Note: Do not add same data again to the Vector store.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e38891",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
