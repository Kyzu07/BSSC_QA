{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSSC_QA Framework - Phase 1: Foundation\n",
    "---\n",
    "**Purpose**: Setup project structure, configuration, and test LLM connectivity\n",
    "\n",
    "**What we'll build**:\n",
    "1. Folder structure creation\n",
    "2. Configuration management (`config.json`)\n",
    "3. LLM factory (Gemini, DeepSeek, Mistral, HuggingFace)\n",
    "4. Vector store setup (ChromaDB)\n",
    "5. Basic connectivity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q langchain>=1.0.0 langchain-chroma>=0.2.0 langchain-google-genai>=2.0.0 \\\n",
    "#     langchain-community>=0.3.0 langchain-huggingface chromadb>=0.5.0 \\\n",
    "#     sentence-transformers>=3.0.0 pydantic>=2.0.0 tiktoken>=0.7.0 python-dotenv>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Project Folder Structure\n",
    "\n",
    "Creates all necessary directories for the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kaizu/Projects/BSSC-QA/bssc_qa/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project structure created at: /home/kaizu/Projects/BSSC-QA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base path\n",
    "base_path = Path.cwd().parent.parent                 #  Or Path.cwd().parent.parent for relative path\n",
    "project_dirs = [\n",
    "    'bssc_qa/src/core',\n",
    "    'bssc_qa/src/agents',\n",
    "    'bssc_qa/src/tools',\n",
    "    'bssc_qa/src/pipeline',\n",
    "    'bssc_qa/src/utils',\n",
    "    'data/chroma_db',\n",
    "    'data/output'\n",
    "]\n",
    "\n",
    "for dir_path in project_dirs:\n",
    "    full_path = base_path / dir_path\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create __init__.py for Python packages\n",
    "    if 'src' in dir_path:\n",
    "        init_file = full_path / '__init__.py'\n",
    "        init_file.touch(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Project structure created at: {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Configuration Schema\n",
    "\n",
    "Generates `config.json` with default settings for all LLM providers and pipeline parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Default configuration\n",
    "# config = {\n",
    "#     \"llm\": {\n",
    "#         \"default_provider\": \"gemini\",\n",
    "#         \"providers\": {\n",
    "#             \"gemini\": {\n",
    "#                 \"api_key\": \"AIzaSyCVuHAUZ7IJjcRb_N0uLGDs4Sj2LPCi3aA\",\n",
    "#                 \"model\": \"gemini-2.5-flash\",\n",
    "#                 \"temperature\": 0.7\n",
    "#             },\n",
    "#             \"deepseek\": {\n",
    "#                 \"api_key\": \"sk-957c94f94c5148ee97104af87170cc0f\",\n",
    "#                 \"model\": \"deepseek-chat\",\n",
    "#                 \"temperature\": 0.7\n",
    "#             },\n",
    "#             \"mistral\": {\n",
    "#                 \"api_key\": \"S0nAqo8LUpiAaHkIYBJL8Zm2IY5nNZWM\",\n",
    "#                 \"model\": \"mistral-large-latest\",\n",
    "#                 \"temperature\": 0.7\n",
    "#             },\n",
    "#             \"huggingface\": {\n",
    "#                 \"api_key\": \"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\",\n",
    "#                 \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "#                 \"temperature\": 0.7\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"vector_store\": {\n",
    "#         \"type\": \"chromadb\",\n",
    "#         \"persist_directory\": \"./data/chroma_db\",\n",
    "#         \"collection_name\": \"bssc_chunks\",\n",
    "#         \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#     },\n",
    "#     \"chunking\": {\n",
    "#         \"chunk_size\": 512,\n",
    "#         \"chunk_overlap\": 50,\n",
    "#         \"auto_adjust\": True\n",
    "#     },\n",
    "#     \"agents\": {\n",
    "#         \"planner\": {\n",
    "#             \"enabled\": False,\n",
    "#             \"provider\": \"gemini\"\n",
    "#         },\n",
    "#         \"generator\": {\n",
    "#             \"provider\": \"gemini\",\n",
    "#             \"max_retries\": 3\n",
    "#         },\n",
    "#         \"synthesis\": {\n",
    "#             \"provider\": \"deepseek\",\n",
    "#             \"context_window\": 3,\n",
    "#             \"max_evidence_spans\": 3\n",
    "#         },\n",
    "#         \"evaluator\": {\n",
    "#             \"provider\": \"mistral\",\n",
    "#             \"quality_threshold\": 0.75,\n",
    "#             \"metrics\": [\"relevance\", \"clarity\", \"completeness\", \"factuality\", \"diversity\"]\n",
    "#         }\n",
    "#     },\n",
    "#     \"bloom_level\": {\n",
    "#         \"enabled\": False,\n",
    "#         \"levels\": [\"remember\", \"understand\", \"apply\", \"analyze\", \"evaluate\", \"create\"]\n",
    "#     },\n",
    "#     \"human_review\": {\n",
    "#         \"enabled\": False,\n",
    "#         \"review_threshold\": 0.6\n",
    "#     },\n",
    "#     \"export\": {\n",
    "#         \"format\": \"json\",\n",
    "#         \"include_metadata\": True,\n",
    "#         \"output_path\": \"./data/output\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Save config\n",
    "# config_path = base_path / 'config.json'\n",
    "# with open(config_path, 'w') as f:\n",
    "#     json.dump(config, indent=2, fp=f)\n",
    "\n",
    "# print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "# print(\"‚ö†Ô∏è  Remember to update API keys in config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Configuration Loading\n",
    "\n",
    "Validates that configuration loads correctly with proper type checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "\n",
      "Default LLM Provider: gemini\n",
      "Vector Store: chromadb\n",
      "Embedding Model: offline-hash\n",
      "Chunk Size: 16000\n",
      "Planner Enabled: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(str(base_path / 'bssc_qa' / 'src'))\n",
    "\n",
    "from core.config import load_config\n",
    "\n",
    "# Load config\n",
    "cfg = load_config(base_path / 'config.json')\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"\\nDefault LLM Provider: {cfg.llm.default_provider}\")\n",
    "print(f\"Vector Store: {cfg.vector_store.type}\")\n",
    "print(f\"Embedding Model: {cfg.vector_store.embedding_model}\")\n",
    "print(f\"Chunk Size: {cfg.chunking.chunk_size}\")\n",
    "print(f\"Planner Enabled: {cfg.agents.planner['enabled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test LLM Factory (Update API Key First!)\n",
    "\n",
    "‚ö†Ô∏è **ACTION REQUIRED**: Update your API key in `config.json` before running this cell.\n",
    "\n",
    "Tests LLM connectivity with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.llm_factory import create_llm\n",
    "\n",
    "# # Reload config (in case you updated API keys)\n",
    "# cfg = load_config(base_path / 'config.json')\n",
    "\n",
    "# # Get default provider config\n",
    "# provider_name = cfg.llm.default_provider\n",
    "# provider_cfg = cfg.llm.providers[provider_name]\n",
    "\n",
    "# # Create LLM\n",
    "# llm = create_llm(\n",
    "#     provider=provider_name,\n",
    "#     api_key=provider_cfg.api_key,\n",
    "#     model=provider_cfg.model,\n",
    "#     temperature=provider_cfg.temperature\n",
    "# )\n",
    "\n",
    "# # Test with simple prompt\n",
    "# response = llm.invoke(\"Say 'Hello from BSSC_QA!' And a pun.\")\n",
    "# print(f\"‚úÖ LLM ({provider_name}) Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Vector Store\n",
    "\n",
    "Initializes ChromaDB and tests document addition/retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_chroma\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete existing ChromaDB (if any)\n",
    "# import shutil\n",
    "# chroma_db_path = base_path / cfg.vector_store.persist_directory\n",
    "# if chroma_db_path.exists():\n",
    "#     shutil.rmtree(chroma_db_path)\n",
    "#     print(f\"üóëÔ∏è  Deleted existing ChromaDB at: {chroma_db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store reset and ready for re-ingestion.\n",
      "‚úÖ Vector Store Test Successful!\n",
      "Added document ID: 423d7917-60b1-472f-8c5e-9e795d5e76b7\n",
      "Retrieved: Bangladesh is known for its rich cultural heritage and natural beauty....\n",
      "Total documents in collection: 1\n"
     ]
    }
   ],
   "source": [
    "from core.vector_store import VectorStoreManager\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Initialize vector store\n",
    "vs_manager = VectorStoreManager(\n",
    "    persist_directory=str(base_path / cfg.vector_store.persist_directory),\n",
    "    collection_name=cfg.vector_store.collection_name,\n",
    "    embedding_model=cfg.vector_store.embedding_model\n",
    ")\n",
    "\n",
    "if True:\n",
    "    vs_manager.reset_collection()\n",
    "    print(\"‚úÖ Vector store reset and ready for re-ingestion.\")\n",
    "\n",
    "\n",
    "# Test document\n",
    "test_doc = Document(\n",
    "    page_content=\"Bangladesh is known for its rich cultural heritage and natural beauty.\",\n",
    "    metadata={\"source\": \"test\", \"chunk_id\": \"test_001\"}\n",
    ")\n",
    "\n",
    "# Add and retrieve\n",
    "doc_ids = vs_manager.add_documents([test_doc])\n",
    "results = vs_manager.similarity_search(\"cultural heritage Bangladesh\", k=1)\n",
    "\n",
    "print(f\"‚úÖ Vector Store Test Successful!\")\n",
    "print(f\"Added document ID: {doc_ids[0]}\")\n",
    "print(f\"Retrieved: {results[0].page_content[:80]}...\")\n",
    "print(f\"Total documents in collection: {vs_manager.get_collection_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Document Loading\n",
    "\n",
    "Tests loading a single travel document from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document loaded: ‡¶™‡ßÅ‡¶ü‡¶®‡ßÄ ‡¶Ü‡¶á‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°.txt\n",
      "Content length: 2524 chars\n",
      "\n",
      "First 200 characters:\n",
      "URL: https://adarbepari.com/putni-island-sundarban\n",
      "‡¶™‡ßÅ‡¶ü‡¶®‡ßÄ ‡¶Ü‡¶á‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°\n",
      "\n",
      "‡¶™‡ßÅ‡¶ü‡¶®‡ßÄ ‡¶Ü‡¶á‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶° (Putni Island) ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ñ‡ßÅ‡¶≤‡¶®‡¶æ ‡¶ú‡ßá‡¶≤‡¶æ‡¶§‡ßá ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶ø‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡ßç‡¶¨‡ßÄ‡¶™ ‡¶Ø‡¶æ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßÄ‡ßü‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶¶‡ßç‡¶¨‡ßÄ‡¶™‡¶ö‡¶∞ ‡¶®‡¶æ‡¶Æ‡ßá‡¶ì ‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡•§ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡¶¨‡¶®‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞ ‡¶™\n"
     ]
    }
   ],
   "source": [
    "from pipeline.document_loaders import load_document\n",
    "from utils.text_processing import normalize_text\n",
    "# Load first travel document\n",
    "travel_dir = base_path / 'data' / 'travel_scraped'\n",
    "sample_file = list(travel_dir.glob('*.txt'))[100]\n",
    "\n",
    "doc_data = load_document(str(sample_file))\n",
    "normalized_content = normalize_text(doc_data['content'])\n",
    "\n",
    "print(f\"‚úÖ Document loaded: {doc_data['metadata']['filename']}\")\n",
    "print(f\"Content length: {len(normalized_content)} chars\")\n",
    "print(f\"\\nFirst 200 characters:\")\n",
    "print(normalized_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Chunking System\n",
    "\n",
    "Tests chunking with the loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document chunked into 201 chunks\n",
      "\n",
      "Chunk 0:\n",
      "  ID: f3e78543-b186-4b5c-a563-90aaabed9c69\n",
      "  Tokens: 631\n",
      "  Position: 0\n",
      "  Text preview: URL: https://adarbepari.com/putni-island-sundarban\n",
      "‡¶™‡ßÅ‡¶ü‡¶®‡ßÄ ‡¶Ü‡¶á‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°\n",
      "\n",
      "‡¶™‡ßÅ‡¶ü‡¶®‡ßÄ ‡¶Ü‡¶á‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶° (Putni Island) ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ñ‡ßÅ‡¶≤‡¶®‡¶æ ‡¶ú‡ßá‡¶≤‡¶æ‡¶§‡ßá ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶ø‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡ßç‡¶¨‡ßÄ‡¶™ ‡¶Ø‡¶æ ‡¶∏‡ßç‡¶•‡¶æ‡¶®...\n"
     ]
    }
   ],
   "source": [
    "from pipeline.chunking import chunk_text\n",
    "\n",
    "# Chunk the document\n",
    "chunks = chunk_text(\n",
    "    normalized_content,\n",
    "    chunk_size=cfg.chunking.chunk_size,\n",
    "    chunk_overlap=cfg.chunking.chunk_overlap,\n",
    "    metadata=doc_data['metadata']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Document chunked into {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk 0:\")\n",
    "print(f\"  ID: {chunks[0].chunk_id}\")\n",
    "print(f\"  Tokens: {chunks[0].tokens}\")\n",
    "print(f\"  Position: {chunks[0].position}\")\n",
    "print(f\"  Text preview: {chunks[0].text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Vector Store & Ingestion Pipeline\n",
    "\n",
    "Sets up ChromaDB and creates ingestion pipeline instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ingestion pipeline ready\n",
      "Current documents in ChromaDB: 1\n"
     ]
    }
   ],
   "source": [
    "from pipeline.ingestion import IngestionPipeline\n",
    "\n",
    "# Initialize vector store\n",
    "vs_manager = VectorStoreManager(\n",
    "    persist_directory=str(base_path / cfg.vector_store.persist_directory),\n",
    "    collection_name=cfg.vector_store.collection_name,\n",
    "    embedding_model=cfg.vector_store.embedding_model\n",
    ")\n",
    "\n",
    "# Create ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    vector_store_manager=vs_manager,\n",
    "    chunk_size=cfg.chunking.chunk_size,\n",
    "    chunk_overlap=cfg.chunking.chunk_overlap\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Ingestion pipeline ready\")\n",
    "print(f\"Current documents in ChromaDB: {vs_manager.get_collection_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Single Document Ingestion\n",
    "\n",
    "Ingests one travel document into ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document ingested successfully!\n",
      "Added 201 chunks to vector store\n",
      "Total documents in ChromaDB: 202\n"
     ]
    }
   ],
   "source": [
    "# Ingest single document\n",
    "sample_file = list(travel_dir.glob('*.txt'))[200]\n",
    "doc_ids = pipeline.ingest_document(str(sample_file))\n",
    "\n",
    "print(f\"‚úÖ Document ingested successfully!\")\n",
    "print(f\"Added {len(doc_ids)} chunks to vector store\")\n",
    "print(f\"Total documents in ChromaDB: {vs_manager.get_collection_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Semantic Search\n",
    "\n",
    "Searches ChromaDB for relevant chunks using similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Search results for: '‡¶¨‡¶æ‡¶ó‡¶æ‡¶®'\n",
      "\n",
      "Result 1:\n",
      "  Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "  Chunk: 82\n",
      "  Tokens: 29\n",
      "  Content: ‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶¨‡¶æ‡¶∏‡ßá ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶¨‡¶æ‡¶∏‡¶∏‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶° ‡¶®‡¶æ‡¶Æ‡¶¨‡ßá‡¶®‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶Ö‡¶ü‡ßã‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶æ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶ì ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßÄ ‡¶¨‡¶ü‡ßá‡¶∂‡ßç‡¶¨‡¶∞ ‡¶è‡¶≤‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ò‡ßÅ‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§...\n",
      "\n",
      "Result 2:\n",
      "  Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "  Chunk: 0\n",
      "  Tokens: 904\n",
      "  Content: URL: https://adarbepari.com/lotkon-narsingdi\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®\n",
      "\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶Æ‡¶æ‡¶®‡ßá‡¶á ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá ‡¶ï‡¶æ‡¶∞‡¶® ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡ßü‡¶™‡ßÅ‡¶∞‡¶æ, ‡¶∂‡¶ø‡¶¨‡¶™‡ßÅ‡¶∞, ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶è‡¶∏‡¶¨ ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡ßü ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶≤‡¶ü‡¶ï‡¶® ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶≤‡¶ü‡¶ï‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶æ‡¶∞‡¶á ‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶´‡¶≤ ‡¶ï‡ßá ‡¶®‡¶æ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡¶ø‡¶®‡¶á ‡¶ï‡¶æ‡¶ú‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶∏‡ßç...\n",
      "\n",
      "Result 3:\n",
      "  Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "  Chunk: 172\n",
      "  Tokens: 7\n",
      "  Content: ‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ò‡ßÅ‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§...\n",
      "\n",
      "Result 4:\n",
      "  Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "  Chunk: 198\n",
      "  Tokens: 0\n",
      "  Content: ‡ßá‡¶®‡•§...\n",
      "\n",
      "Result 5:\n",
      "  Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "  Chunk: 199\n",
      "  Tokens: 0\n",
      "  Content: ‡¶®‡•§...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for relevant chunks\n",
    "query = \"‡¶¨‡¶æ‡¶ó‡¶æ‡¶®\"\n",
    "results = vs_manager.similarity_search(query, k=5)\n",
    "\n",
    "print(f\"‚úÖ Search results for: '{query}'\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Source: {result.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"  Chunk: {result.metadata.get('position', 'N/A')}\")\n",
    "    print(f\"  Tokens: {result.metadata.get('tokens', 'N/A')}\")\n",
    "    print(f\"  Content: {result.page_content[:250]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Ingest Travel Documents (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This will process multiple documents. Start with a small number for testing.\n",
    "\n",
    "Ingests multiple travel documents. Adjust `max_files` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch ingest (start with 10 files for testing)\n",
    "# results = pipeline.ingest_directory(\n",
    "#     directory=travel_dir,\n",
    "#     pattern='*.txt',\n",
    "#     max_files=10  # Increase this number as needed\n",
    "# )\n",
    "\n",
    "# print(f\"\\n‚úÖ Batch ingestion complete!\")\n",
    "# print(f\"\\nResults:\")\n",
    "# print(f\"  Total files: {results['total_files']}\")\n",
    "# print(f\"  Processed: {results['processed']}\")\n",
    "# print(f\"  Failed: {results['failed']}\")\n",
    "# print(f\"  Total chunks: {results['total_chunks']}\")\n",
    "# print(f\"\\nChromaDB total documents: {vs_manager.get_collection_count()}\")\n",
    "\n",
    "# if results['errors']:\n",
    "#     print(f\"\\n‚ö†Ô∏è Errors:\")\n",
    "#     for error in results['errors'][:3]:  # Show first 3 errors\n",
    "#         print(f\"  {error['file']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Ingestion Statistics\n",
    "\n",
    "Shows detailed statistics about the ingested documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ingestion Statistics:\n",
      "\n",
      "ChromaDB:\n",
      "  Total chunks: 202\n",
      "  Collection: bssc_chunks\n",
      "\n",
      "Chunking:\n",
      "  Chunk size: 16000 tokens\n",
      "  Chunk overlap: 50 tokens\n",
      "  Avg tokens (sample): 18\n",
      "\n",
      "Embedding Model:\n",
      "  offline-hash\n"
     ]
    }
   ],
   "source": [
    "# Get sample chunks to analyze\n",
    "sample_results = vs_manager.similarity_search(\"travel destination\", k=5)\n",
    "\n",
    "# Calculate statistics\n",
    "total_tokens = sum(r.metadata.get('tokens', 0) for r in sample_results)\n",
    "avg_tokens = total_tokens / len(sample_results) if sample_results else 0\n",
    "\n",
    "print(f\"‚úÖ Ingestion Statistics:\")\n",
    "print(f\"\\nChromaDB:\")\n",
    "print(f\"  Total chunks: {vs_manager.get_collection_count()}\")\n",
    "print(f\"  Collection: {cfg.vector_store.collection_name}\")\n",
    "print(f\"\\nChunking:\")\n",
    "print(f\"  Chunk size: {cfg.chunking.chunk_size} tokens\")\n",
    "print(f\"  Chunk overlap: {cfg.chunking.chunk_overlap} tokens\")\n",
    "print(f\"  Avg tokens (sample): {avg_tokens:.0f}\")\n",
    "print(f\"\\nEmbedding Model:\")\n",
    "print(f\"  {cfg.vector_store.embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Agent ToolsWe'll build the tools that agents will use: retrieval from ChromaDB, chunk analysis, and QA validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval Tool Test\n",
      "\n",
      "Query: '‡¶ï‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ü‡ßá‡¶ï ‡¶¨‡¶æ‡¶∏‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°'\n",
      "\n",
      "Retrieved Context:\n",
      "[Chunk 1]\n",
      "Source: ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®.txt\n",
      "Position: 0\n",
      "Content: URL: https://adarbepari.com/lotkon-narsingdi\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®\n",
      "\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶Æ‡¶æ‡¶®‡ßá‡¶á ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá ‡¶ï‡¶æ‡¶∞‡¶® ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡ßü‡¶™‡ßÅ‡¶∞‡¶æ, ‡¶∂‡¶ø‡¶¨‡¶™‡ßÅ‡¶∞, ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶è‡¶∏‡¶¨ ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡ßü ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶≤‡¶ü‡¶ï‡¶® ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶≤‡¶ü‡¶ï‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶æ‡¶∞‡¶á ‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶´‡¶≤ ‡¶ï‡ßá ‡¶®‡¶æ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡¶ø‡¶®‡¶á ‡¶ï‡¶æ‡¶ú‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶∏‡ßç‡¶§ ‡¶•‡¶æ‡¶ï‡ßá‡¶® ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß ‡¶¶‡¶ø‡¶®‡ßá‡¶∞ ‡¶ñ‡ßÅ‡¶¨ ‡¶Ü‡¶∞‡¶æ‡¶Æ‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶è‡¶ï‡¶ü‡¶æ ‡¶≠‡ßç‡¶∞‡¶Æ‡¶®‡•§ ‡¶§‡¶¨‡ßá ‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®‡ßá‡¶∞ ‡¶™‡¶æ‡¶∂‡¶æ‡¶™‡¶æ‡¶∂‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Æ‡¶® ‡¶≠‡¶∞‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶æ‡¶ü‡¶ø‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶¨‡¶æ‡ßú‡¶ø, ‡¶ö‡¶æ‡¶∞‡¶ø‡¶¶‡¶ø‡¶ï‡ßá‡¶∞ ‡¶∂‡¶æ‡¶®‡ßç‡¶§ ‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂ ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã‡¶§‡ßã ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ‡¶∞ ‡¶™‡¶æ‡¶∂‡¶æ‡¶™‡¶æ‡¶∂‡¶ø ‡¶®‡¶æ‡¶®‡¶æ ‡¶ß‡¶∞‡¶®‡ßá...\n"
     ]
    }
   ],
   "source": [
    "from tools.retrieval_tool import RetrievalTool\n",
    "\n",
    "# Initialize retrieval tool with existing vector store\n",
    "retrieval_tool = RetrievalTool(vs_manager)\n",
    "\n",
    "# Test retrieval\n",
    "query = \"‡¶ï‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ü‡ßá‡¶ï ‡¶¨‡¶æ‡¶∏‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°\"\n",
    "context = retrieval_tool.retrieve_context(query, k=2)\n",
    "\n",
    "print(\"‚úÖ Retrieval Tool Test\")\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"\\nRetrieved Context:\\n{context[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation Tool Test\n",
      "\n",
      "Test Case 1:\n",
      "  Question: What is the Oxford Mission Church?...\n",
      "  Overall Score: 1.00\n",
      "  Passed: True\n",
      "  Flags: []\n",
      "\n",
      "Test Case 2:\n",
      "  Question: Where...\n",
      "  Overall Score: 0.34\n",
      "  Passed: False\n",
      "  Flags: ['question_too_short', 'answer_too_short', 'missing_question_mark', 'answer_too_brief']\n",
      "\n",
      "Test Case 3:\n",
      "  Question: Why should tourists visit Bangladesh...\n",
      "  Overall Score: 0.84\n",
      "  Passed: True\n",
      "  Flags: ['missing_question_mark']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.validation_tool import ValidationTool\n",
    "\n",
    "# Initialize validation tool\n",
    "validator = ValidationTool(quality_threshold=0.75)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What is the Oxford Mission Church?\",\n",
    "        \"answer\": \"The Oxford Mission Church is the second largest church in Asia and one of the unique architectural landmarks in Bangladesh. It is located in Barisal and features beautiful artistic design.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where\",  # Bad question\n",
    "        \"answer\": \"Place\"  # Bad answer\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why should tourists visit Bangladesh\",  # Missing question mark\n",
    "        \"answer\": \"Bangladesh offers rich cultural heritage, natural beauty, and historical sites that attract visitors from around the world.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Validation Tool Test\\n\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = validator.validate_qa(\n",
    "        test['question'], \n",
    "        test['answer']\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"  Question: {test['question'][:50]}...\")\n",
    "    print(f\"  Overall Score: {result.overall_score:.2f}\")\n",
    "    print(f\"  Passed: {result.passed}\")\n",
    "    print(f\"  Flags: {result.flags}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk Analysis Tool Test\n",
      "\n",
      "Chunk Preview: URL: https://adarbepari.com/lotkon-narsingdi\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®\n",
      "\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶Æ‡¶æ‡¶®‡ßá‡¶á ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá ‡¶ï‡¶æ‡¶∞‡¶® ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡ßü‡¶™‡ßÅ‡¶∞‡¶æ, ‡¶∂‡¶ø‡¶¨‡¶™‡ßÅ‡¶∞, ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶è‡¶∏‡¶¨ ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡ßü ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶≤‡¶ü‡¶ï‡¶® ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶≤‡¶ü‡¶ï‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶æ‡¶∞‡¶á ‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶´‡¶≤ ‡¶ï‡ßá ‡¶®‡¶æ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡¶ø‡¶®‡¶á ‡¶ï‡¶æ‡¶ú‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶∏‡ßç‡¶§ ‡¶•‡¶æ‡¶ï‡ßá‡¶® ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß ‡¶¶‡¶ø‡¶®‡ßá‡¶∞ ‡¶ñ‡ßÅ‡¶¨ ‡¶Ü‡¶∞‡¶æ‡¶Æ‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶è‡¶ï‡¶ü‡¶æ ‡¶≠‡ßç‡¶∞‡¶Æ...\n",
      "\n",
      "Analysis Results:\n",
      "  Sentences: 41\n",
      "  Words: 569\n",
      "  Entities: ['‡¶∏‡¶¨‡¶æ‡¶∞‡¶á', '‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡¶ú‡¶®‡¶ï', '‡¶¨‡¶æ‡¶ó‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤', '‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤', '‡¶Ö‡¶®‡ßá‡¶ï']\n",
      "  Has Numbers: True\n",
      "  Number Count: 15\n",
      "\n",
      "Suggested Question Types: ['factual', 'quantitative', 'conceptual', 'analytical']\n"
     ]
    }
   ],
   "source": [
    "from tools.chunk_tool import ChunkAnalysisTool\n",
    "\n",
    "# Initialize chunk analysis tool\n",
    "chunk_analyzer = ChunkAnalysisTool()\n",
    "\n",
    "# Get a sample chunk from vector store\n",
    "sample_chunks = vs_manager.similarity_search(\"‡¶ï‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ü‡ßá‡¶ï ‡¶¨‡¶æ‡¶∏‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°\", k=5)\n",
    "sample_text = sample_chunks[0].page_content\n",
    "\n",
    "# Analyze the chunk\n",
    "analysis = chunk_analyzer.analyze_chunk(sample_text)\n",
    "suggested_types = chunk_analyzer.suggest_question_types(analysis)\n",
    "\n",
    "print(\"‚úÖ Chunk Analysis Tool Test\\n\")\n",
    "print(f\"Chunk Preview: {sample_text[:300]}...\\n\")\n",
    "print(f\"Analysis Results:\")\n",
    "print(f\"  Sentences: {analysis['sentence_count']}\")\n",
    "print(f\"  Words: {analysis['word_count']}\")\n",
    "print(f\"  Entities: {analysis['entities'][:5]}\")\n",
    "print(f\"  Has Numbers: {analysis['has_numbers']}\")\n",
    "print(f\"  Number Count: {analysis['number_count']}\")\n",
    "print(f\"\\nSuggested Question Types: {suggested_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph already installed\n"
     ]
    }
   ],
   "source": [
    "# Check if we need to install anything\n",
    "try:\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    print(\"‚úÖ LangGraph already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing langgraph...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'langgraph>=0.2.0'])\n",
    "    print(\"‚úÖ LangGraph installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing Generator Agent\n",
      "\n",
      "Sample Chunk Preview:\n",
      "URL: https://adarbepari.com/lotkon-narsingdi\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶¨‡¶æ‡¶ó‡¶æ‡¶®\n",
      "\n",
      "‡¶≤‡¶ü‡¶ï‡¶® ‡¶Æ‡¶æ‡¶®‡ßá‡¶á ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá ‡¶ï‡¶æ‡¶∞‡¶® ‡¶®‡¶∞‡¶∏‡¶ø‡¶Ç‡¶¶‡ßÄ ‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡ßü‡¶™‡ßÅ‡¶∞‡¶æ, ‡¶∂‡¶ø‡¶¨‡¶™‡ßÅ‡¶∞, ‡¶Æ‡¶∞‡¶ú‡¶æ‡¶≤ ‡¶è‡¶∏‡¶¨ ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡ßü ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶≤‡¶ü‡¶ï‡¶® ‡¶π‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶≤‡¶ü‡¶ï‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶æ‡¶∞‡¶á ‡¶™...\n",
      "\n",
      "Generating questions...\n",
      "\n",
      "‚úÖ Generated 3 questions:\n",
      "\n",
      "Q1: Which specific upazilas and villages in Narsingdi are noted for having the highest concentration of Lotkon trees, and what is the typical price range per kilogram for Lotkon when purchased directly from the gardens?\n",
      "   Type: factual\n",
      "   Rationale: This question tests the recall of key geographical locations and practical purch...\n",
      "\n",
      "Q2: Beyond the opportunity to pick and eat Lotkon, what other attractions and environmental aspects are highlighted in the text as contributing to a pleasant and appealing one-day trip to the Lotkon gardens in Narsingdi?\n",
      "   Type: conceptual\n",
      "   Rationale: This question encourages the user to synthesize information about the overall ex...\n",
      "\n",
      "Q3: Compare the two primary public transportation routes described for reaching the Lotkon gardens near Ajkitola market in Shibpur from Dhaka, detailing the starting points in Dhaka, the type of transport, and the estimated costs for each segment of the journey.\n",
      "   Type: analytical\n",
      "   Rationale: This question requires the user to analyze and compare different travel logistic...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from core.llm_factory import create_llm\n",
    "from agents.generator_agent import GeneratorAgent\n",
    "from tools.chunk_tool import ChunkAnalysisTool\n",
    "\n",
    "# Setup LLM\n",
    "provider_cfg = cfg.llm.providers[cfg.llm.default_provider]\n",
    "llm = create_llm(\n",
    "    provider=cfg.llm.default_provider,\n",
    "    api_key=provider_cfg.api_key,\n",
    "    model=provider_cfg.model,\n",
    "    temperature=provider_cfg.temperature\n",
    ")\n",
    "\n",
    "# Initialize tools\n",
    "chunk_analyzer = ChunkAnalysisTool()\n",
    "retrieval_tool = RetrievalTool(vs_manager)\n",
    "\n",
    "# Create generator agent\n",
    "generator = GeneratorAgent(llm, retrieval_tool, chunk_analyzer)\n",
    "\n",
    "# Get sample chunk\n",
    "sample_results = vs_manager.similarity_search(\"‡¶ï‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ü‡ßá‡¶ï ‡¶¨‡¶æ‡¶∏‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°\", k=1)\n",
    "sample_chunk = sample_results[0].page_content\n",
    "\n",
    "print(\"‚úÖ Testing Generator Agent\\n\")\n",
    "print(f\"Sample Chunk Preview:\\n{sample_chunk[:200]}...\\n\")\n",
    "print(\"Generating questions...\")\n",
    "\n",
    "# Generate questions\n",
    "questions = generator.generate_questions(sample_chunk, count=3)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(questions)} questions:\\n\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q['question']}\")\n",
    "    print(f\"   Type: {q['question_type']}\")\n",
    "    print(f\"   Rationale: {q['rationale'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaizu/Projects/BSSC-QA/bssc_qa/src/core/llm_factory.py:24: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import ChatOpenAI``.\n",
      "  return ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing Synthesis Agent\n",
      "\n",
      "Question: Which specific upazilas and villages in Narsingdi are noted for having the highest concentration of Lotkon trees, and what is the typical price range per kilogram for Lotkon when purchased directly from the gardens?\n",
      "\n",
      "Synthesizing answer...\n",
      "\n",
      "‚úÖ Answer Generated:\n",
      "Answer: Based on the provided evidence, a specific answer to the question cannot be provided.\n",
      "\n",
      "The evidence does not contain the necessary information to identify the specific upazilas and villages in Narsingdi with the highest concentration of Lotkon trees, nor does it mention the typical price range per k...\n",
      "\n",
      "Evidence spans used: 3\n"
     ]
    }
   ],
   "source": [
    "from agents.synthesis_agent import SynthesisAgent\n",
    "\n",
    "# Create synthesis agent\n",
    "synthesis_cfg = cfg.agents['synthesis']\n",
    "synthesis_llm = create_llm(\n",
    "    provider=synthesis_cfg['provider'],\n",
    "    api_key=cfg.llm.providers[synthesis_cfg['provider']].api_key,\n",
    "    model=cfg.llm.providers[synthesis_cfg['provider']].model,\n",
    "    temperature=cfg.llm.providers[synthesis_cfg['provider']].temperature\n",
    ")\n",
    "\n",
    "synthesizer = SynthesisAgent(\n",
    "    synthesis_llm,\n",
    "    vs_manager,\n",
    "    max_evidence_spans=synthesis_cfg['max_evidence_spans']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Testing Synthesis Agent\\n\")\n",
    "\n",
    "# Synthesize answer for first question\n",
    "if questions:\n",
    "    test_question = questions[0]['question']\n",
    "    print(f\"Question: {test_question}\\n\")\n",
    "    print(\"Synthesizing answer...\")\n",
    "    \n",
    "    qa_pair = synthesizer.synthesize_answer(\n",
    "        test_question,\n",
    "        questions[0]['question_type']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer Generated:\")\n",
    "    print(f\"Answer: {qa_pair['answer'][:300]}...\")\n",
    "    print(f\"\\nEvidence spans used: {len(qa_pair['evidence_spans'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing Evaluator Agent\n",
      "\n",
      "Evaluating QA pair...\n",
      "Error in evaluation: Error code: 404 - {'message': 'no Route matched with those values', 'request_id': '5831aa3c48a69a65aca9696f17142773'}\n",
      "\n",
      "‚úÖ Evaluation Results:\n",
      "Overall Score: 1.00\n",
      "Passed: True\n",
      "\n",
      "Detailed Scores:\n",
      "  length: 1.00\n",
      "  answer_length: 1.00\n",
      "  format: 1.00\n",
      "  relevance: 1.00\n",
      "  completeness: 1.00\n",
      "\n",
      "Flags: ['llm_evaluation_failed']\n"
     ]
    }
   ],
   "source": [
    "from agents.evaluator_agent import EvaluatorAgent\n",
    "from tools.validation_tool import ValidationTool\n",
    "\n",
    "# Create evaluator agent\n",
    "evaluator_cfg = cfg.agents['evaluator']\n",
    "evaluator_llm = create_llm(\n",
    "    provider=evaluator_cfg['provider'],\n",
    "    api_key=cfg.llm.providers[evaluator_cfg['provider']].api_key,\n",
    "    model=cfg.llm.providers[evaluator_cfg['provider']].model,\n",
    "    temperature=cfg.llm.providers[evaluator_cfg['provider']].temperature\n",
    ")\n",
    "\n",
    "validator = ValidationTool(quality_threshold=evaluator_cfg['quality_threshold'])\n",
    "evaluator = EvaluatorAgent(evaluator_llm, validator, evaluator_cfg['quality_threshold'])\n",
    "\n",
    "print(\"‚úÖ Testing Evaluator Agent\\n\")\n",
    "\n",
    "# Evaluate the QA pair\n",
    "if qa_pair:\n",
    "    print(\"Evaluating QA pair...\")\n",
    "    evaluation = evaluator.evaluate_qa(qa_pair)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation Results:\")\n",
    "    print(f\"Overall Score: {evaluation['overall_score']:.2f}\")\n",
    "    print(f\"Passed: {evaluation['passed']}\")\n",
    "    print(f\"\\nDetailed Scores:\")\n",
    "    for metric, score in evaluation['scores'].items():\n",
    "        print(f\"  {metric}: {score:.2f}\")\n",
    "    if evaluation['flags']:\n",
    "        print(f\"\\nFlags: {evaluation['flags']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
